---
begin: 2022-01-09
status: ongoing
rating: 1
---

# 分布式链路追踪论文-Dapper阅读笔记

## 简介
Dapper 是谷歌内部使用的分布式链路追踪系统。谷歌技术人员于 2010 年将 Dapper 的设计思想以及工程实践作为论文发表，后续大量分布式链路系统均能看到 Dapper 的影子。

## 1 为什么需要分布式链路追踪

现代互联网系统多运行在成千上万机器支撑的分布式系统之上，一个复杂的业务请求往往需要多个服务共同配合完成。如果系统异常或者响应特别慢，定位问题往往需要查看所有请求经过服务的日志，如同大海捞针非常耗费人力，并且只有经验丰富且对系统非常熟悉的开发人员才能够快速定位到问题。互联网项目分秒必争，快人一步就能击败竞争对手，而如果系统出现线上故障，故障每延续一秒都会造成巨大的经济损失并且极大的危害公司的声誉。
因此通过分布式链路追踪定位故障原因以及分析性能瓶颈是非常有必要的。Dapper在谷歌内部起初是一个纯粹的链路追踪工具，后期功能逐渐丰富成为了一个强大的监控平台。

## 2 Dapper 设计目标与实现思路
Dapper 的三大核心设计目标：
- 全面部署：只有全系统覆盖才能保证调用链路的完整性
- 低延迟：链路追踪无可避免会增加系统开销，影响系统性能，因此链路追踪应该尽量少的影响性能
- 应用级别透明（最困难）：依赖业务代码串联链路是不切实际的，为了能高效快速的落地，一定要应用级别透明，减少集成的开发工作量
此外 Dapper 还希望能够以最快的速度收集到整条链路，并且提供可视化效果优秀的 UI 界面

Dapper 针对上述目标的核心实现思路：
- 通过控制采样率可以极大的减少延迟
- 通过将链路追踪的代码汇集到公告基础库，与业务系统无关联。前提是架构层面的统一，例如所有服务都使用同一套 RPC 框架

## 3 Dapper 总体设计

来看图1从而对链路追踪有个直观的了解，首先链路是一个树形结构。图中用户从前端 A 发起两个 RPC 调用后端服务 B 和服务 C，其中后端服务 C 又调用了服务 D 和服务Ｅ。图一示例中仅有 RPC 调用，根据实际情况 HTTP 、数据库、缓存、消息队列或者异步方法等调用都可以加入到链路中来。

![图1](image/Pasted%20image%2020220112005328.png)


图1描述了服务调用的依赖关系，为了更好地分析性能还需要记录所有请求耗时等信息。此外图一是更利于理解，我们把调用链树称为 Trace。Trace 拥有全局唯一 traceId。Trace 上的每个节点也就是一次调用称为一个跨度Span，Span 也拥有全局唯一 spanId，Trace 的每条边表示 Span 之间的关系，每个 Span 需要将其父节点的 spanId  保存在 parentId 字段中，并且保存统一的 traceId。如图二所示更贴近计算机实现。并且从图二已经能很好的看出系统的耗时情况，也指导了追踪系统 UI 的设计。

![图二](image/Pasted%20image%2020220112085033.png)

### 3.1 跨度 Span 详细设计

每个 Span 其实就是调用的一段精简的日志，Span 中保存的信息应该包含
- 上文中提到的  traceId，spanId 和 parentId。Dapper 中所有 ID 字段是全局唯一的 64 位整数
- 开始和结束的时间戳
- 一眼就能定位到调用的 Span 名
- 便于定位分析问题的特定应用注解补充信息

此外每个 Span 的记录分两阶段，






## 参考链接


##### 标签
#trace 